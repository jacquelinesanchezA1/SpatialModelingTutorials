{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jshogland/SpatialModelingTutorials/blob/main/Notebooks/ShawneeNF_TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv69joTwdFaA"
      },
      "source": [
        "# Percent Tree Canopy Cover Notebook\n",
        "## In this notebook we will explore how to build a simple random sample of point locations, extract remotely sensed data for those locations, and use that data to estimate % Tree Cover for the Shawnee National Forest.\n",
        "\n",
        "### Project objective:\n",
        "- Build a machine learning model from estimates of 2016 percent forest canopy cover, elevation data, and Landsat 8 imagery for that same time period\n",
        "- Apply that model to elevation and Landsat 8 data acquired in 2021 to estimate percent forest canopy cover in 2021\n",
        "- Compare our model estimates to [MLRC](https://www.mrlc.gov/data-services-page) % forest canopy cover estimates for the year 2021\n",
        "\n",
        "#### Notebook Sections:\n",
        "1. Setup\n",
        "2. Create a Sample\n",
        "3. Download data\n",
        "4. Creating the Data Frame\n",
        "5. Clean data\n",
        "6. Preprocessing\n",
        "7. Build a predictive model\n",
        "8. Estimate Percent Tree Canopy Cover\n",
        "\n",
        "#### Data sources\n",
        "- [Landsat 8](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) - Extracted from Planetary Computer ([STAC](https://stacspec.org/en))\n",
        "- [Tree Canopy Cover 2016](https://apps.fs.usda.gov/fsgisx01/rest/services/RDW_LandscapeAndWildlife/NLCD_2016_TreeCanopyCover_CONUS/ImageServer) - Extracted from Landfire Image Service ([REST](https://developers.arcgis.com/rest/services-reference/enterprise/image-service/))\n",
        "- [USGS Elevation](https://www.usgs.gov/3d-elevation-program) - Extracted from USGS 3DEP program ([py3dep](https://github.com/hyriver/py3dep))\n",
        "- [National Forest Boundary](https://www.openstreetmap.org/#map=9/34.306/-112.242) - Extracted from Open Street Maps ([osmnx](https://osmnx.readthedocs.io/en/stable/))\n",
        "\n",
        "Author John Hogland 10/1/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdouEjUmdFaB"
      },
      "source": [
        "### Section 1: Setup\n",
        "#### Installing software for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdqdQ1_PdFaB"
      },
      "outputs": [],
      "source": [
        "!pip install mapclassify\n",
        "!pip install osmnx\n",
        "!pip install raster_tools\n",
        "!pip install planetary-computer\n",
        "!pip install pystac-client\n",
        "!pip install stackstac\n",
        "!pip install py3dep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIuHENindFaB"
      },
      "source": [
        "#### Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTkFqp2LdFaC"
      },
      "outputs": [],
      "source": [
        "#get packages\n",
        "import osmnx as ox, planetary_computer, pystac_client, stackstac\n",
        "import geopandas as gpd, pandas as pd, os, numpy as np, requests, urllib, py3dep\n",
        "\n",
        "from raster_tools import Raster,zonal, general, Vector\n",
        "from shapely import geometry\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "uGSrvI_VdFaC"
      },
      "source": [
        "### Section 2: Create a sample\n",
        "#### In this section we will extract the spatial boundary of the Shawnee National Forest using Open Street Maps (OSM) Web Service and create a simple random sample of locations within the boundary of the National forest.\n",
        "\n",
        "#### Key learning points:\n",
        "- Using OSM to get vector data (downloading)\n",
        "- Generating a simple random sample of point location that will be used to create a data frame of response and predictor variables\n",
        "- How to Visualize the the boundary and point locations\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Use osmnx and OSM to get a GeoDataFrame (polygons) of the boundary of the Shawnee National Forest\n",
        "2. Use the Shawnee National Forest boundary and Geopanda's sample_points function to create a simple random sample of point locations\n",
        "3. Create a interactive map that displays our point locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJhsSKWPdFaC"
      },
      "source": [
        "##### Step 1: Get the Shawnee National Forest boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-OrUepEdFaC"
      },
      "outputs": [],
      "source": [
        "#use osmnx OpenStreetMaps to get the boundary of the NF (GeoDataFrame)\n",
        "nf=ox.geocode_to_gdf('Shawnee National Forest, IL, USA')\n",
        "\n",
        "#Look at the coordinate system of the GeoDataFrame\n",
        "print('Original CRS =',nf.crs)\n",
        "\n",
        "#project the GeoDataFrame to Albers equal area EPSG:5070\n",
        "nfp=nf.to_crs(5070)\n",
        "\n",
        "#Look at the GeoDataFrame\n",
        "display(nfp)\n",
        "\n",
        "#Plot the national forest\n",
        "nfp.plot(figsize=(15,15))\n",
        "\n",
        "#Why are we projecting to Albers equal Area?\n",
        "\n",
        "#How many acres is the National Forest?\n",
        "print('Number of acres =',(nfp.area * 0.000247105).values[0])\n",
        "\n",
        "#How many polygons are in the National Forest?\n",
        "print('Number of multipolygons =',nfp.shape[0])\n",
        "print('Number of polygons =',nfp.explode().shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SmIuf2dFaC"
      },
      "source": [
        "##### Step 3: Create the simple random sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GT6iqyxdFaC"
      },
      "outputs": [],
      "source": [
        "#us random sample function to create 2,000 locations within the nfp\n",
        "mpnts=nfp.sample_points(2000)\n",
        "\n",
        "#Look at the GeoSeries\n",
        "mpnts\n",
        "\n",
        "#Why 2,000 locations? Why not 100 or 10.000?\n",
        "\n",
        "#How many records do we have?\n",
        "#print('Number of records =',mpnts.shape[0])\n",
        "\n",
        "#How many observations do we have?\n",
        "#print('Number of points (observations) =',mpnts.explode().shape[0])\n",
        "\n",
        "#How can we convert our multipoint to points?\n",
        "pnts=mpnts.explode().reset_index(drop=True) #reseting the index to clean things up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elx0kt__dFaC"
      },
      "source": [
        "##### Step 4: Visualize the national forest boundary and point locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHmkhbhXdFaD"
      },
      "outputs": [],
      "source": [
        "#Use geopandas explore function to create a interactive map\n",
        "m=nfp.explore(color='blue') #create the map using the National Forest Boundary\n",
        "m=pnts.explore(m=m,color='yellow') #add out points to the map\n",
        "m #show the map\n",
        "\n",
        "#Can we save our map to share with others?\n",
        "#m.save(sample.html)\n",
        "\n",
        "#How can we plot our map?\n",
        "# p=nfp.plot(edgecolor='blue',facecolor='none',figsize=(15,15))\n",
        "# p=pnts.plot(ax=p,color='yellow')\n",
        "# p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-9HKYGdFaD"
      },
      "source": [
        "### Section 3: Downloading Raster Data\n",
        "#### In this section we will focus on download raster dataset from various sources.\n",
        "\n",
        "#### Key learning points:\n",
        "- Accessing data from different web services\n",
        "- Deciding where to do your processing (client or server side)\n",
        "- How and when to store the data locally\n",
        "- How to visualizing the raster surfaces\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Create functions to download data from STAC, REST, WCS\n",
        "2. Use those functions to create Raster datasets\n",
        "3. Visualize Your Raster datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4qM4HJdFaD"
      },
      "source": [
        "##### Step 1: Creating download definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMe0XcOadFaD"
      },
      "outputs": [],
      "source": [
        "from owslib.wcs import WebCoverageService\n",
        "#create definition to mosaic stac data\n",
        "def mosaic_stac(xr):\n",
        "    '''\n",
        "    Creates a mosaic from multi-temporal xarray stack\n",
        "\n",
        "    xr=Xarray object\n",
        "\n",
        "    returns xarray mosaic\n",
        "    '''\n",
        "    return stackstac.mosaic(xr)\n",
        "\n",
        "#create definition to extract stac data\n",
        "def get_stac_data(ply,url,name,bands,res=30,crs=5070,\n",
        "                  out_prefix='ls8',**kwarg):\n",
        "    '''\n",
        "    gets tiled data from planetary computer as a dask backed xarray that intersects the geometry of the point, line, or polygon\n",
        "\n",
        "    ply = (geoseries or geodataframe) of the study area\n",
        "    url = (string) base url to planetary computer https://planetarycomputer.microsoft.com/api/stac/v1\n",
        "    name = (string) catelog resource e.g., \"sentinel-2-l2a\"\n",
        "    bands = (list(string)) of attributes ['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
        "    qry =  (dictionary) of property values {'eo:cloud_cover':{'lt':1}}\n",
        "    res = (tuple of numbers) output resolution (x,y)\n",
        "    crs = (int) output crs\n",
        "    dt = (string) data time intervale e.g., one month: 2023-06, range: 2023-06-02/2023-06-17\n",
        "    limit = (int) max number of items to return\n",
        "    out_prefix = (string) prefix used to save the image\n",
        "\n",
        "    returns a Raster object\n",
        "    '''\n",
        "    geo = ply.to_crs('4326').envelope.geometry[0]\n",
        "    xmin,ymin,xmax,ymax=ply.total_bounds\n",
        "    catalog = pystac_client.Client.open(url, modifier=planetary_computer.sign_inplace)\n",
        "    srch = catalog.search(collections=name, intersects=geo, **kwarg)\n",
        "    ic = srch.item_collection()\n",
        "    if(len(ic.items)>0):\n",
        "        xra = stackstac.stack(ic,resolution=res,epsg=crs)\n",
        "        xra = mosaic_stac(xra)\n",
        "        rs=Raster(xra.sel(band=bands,x=slice(xmin,xmax),y=slice(ymax,ymin)))\n",
        "        outpath=out_prefix+'.tif'\n",
        "        rs.save(outpath)\n",
        "        rs=Raster(outpath)\n",
        "    else:\n",
        "        rs=None\n",
        "\n",
        "    return rs\n",
        "\n",
        "#Create definition to extract image service data\n",
        "def get_image_service_data(url, ply, out_prefix,res=30,outSR=\"\"):\n",
        "    '''\n",
        "    extracts a list of images from a image service given a url, polygon, and output prefix name\n",
        "\n",
        "    url = (string) path to image service e.g., url=r'https://lfps.usgs.gov/arcgis/rest/services/Landfire_LF230/US_230EVT/ImageServer'\n",
        "    ply = (geoseries or geodataframe) of the study area\n",
        "    out_prefix = (string) prefix used to save each image\n",
        "\n",
        "    returns a list of Raster objects, one for each tile\n",
        "    '''\n",
        "    layerInfo=requests.get(url+'?f=pjson')\n",
        "    dic=layerInfo.json()\n",
        "    #print(dic)\n",
        "    spr=dic['spatialReference']\n",
        "    m_width=dic['maxImageWidth']\n",
        "    m_height=dic['maxImageHeight']\n",
        "    fitem=next(iter(spr))\n",
        "    ply2=ply.to_crs(spr[fitem])\n",
        "\n",
        "    xmin,ymin,xmax,ymax=ply2.total_bounds\n",
        "\n",
        "    wcells=int((xmax-xmin)/res)\n",
        "    hcells=int((ymax-ymin)/res)\n",
        "\n",
        "    if(wcells<m_width):\n",
        "        m_width=wcells\n",
        "\n",
        "    if(hcells<m_height):\n",
        "        m_height=hcells\n",
        "\n",
        "\n",
        "    wcells_l=np.arange(0,wcells,m_width)\n",
        "    hcells_l=np.arange(0,hcells,m_height)\n",
        "\n",
        "    xmax2=xmin\n",
        "    ymax2=ymin\n",
        "\n",
        "    tile=1\n",
        "\n",
        "    rs_lst=[]\n",
        "    for w in wcells_l:\n",
        "        for h in hcells_l:\n",
        "            xmax2 = (m_width*res+xmax2)\n",
        "            ymax2 = (m_height*res+ymax2)\n",
        "\n",
        "            qry = url+'/exportImage?'\n",
        "            parm = {\n",
        "                'f':'json',\n",
        "                'bbox':','.join([str(xmin),str(ymin),str(xmax2),str(ymax2)]),\n",
        "                'size':str(m_width) + ',' + str(m_height),\n",
        "                'imageSR':outSR,\n",
        "                'format':'tiff'\n",
        "            }\n",
        "            #print(parm['bbox'])\n",
        "            response=requests.get(qry,parm)\n",
        "            if response.status_code == 200:\n",
        "                img_url=response.json()['href']\n",
        "                outname=out_prefix + str(tile) + '.tif'\n",
        "                urllib.request.urlretrieve(img_url, outname)\n",
        "                rs_lst.append(Raster(outname))\n",
        "                tile+=1\n",
        "\n",
        "    return rs_lst\n",
        "\n",
        "# Creat definition for WCS download\n",
        "def get_wcs_data(url,ply,service_name='mrlc_download__nlcd_tcc_conus_2021_v2021-4',out_prefix = 'tcc'):\n",
        "    '''\n",
        "    Extracts saves an image from a WCS given url, polygon boundary, and service name. Images are saved in the same location as the notebook.\n",
        "    url = (string) path to wcs e.g. 'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
        "    ply = (geoseries or geodataframe) of the study area\n",
        "    service_name = (string) name of the service e.g. mrlc_download__nlcd_tcc_conus_2021_v2021-4\n",
        "    out_prefix = (string) prefix used to save each image\n",
        "\n",
        "    returns a Raster object\n",
        "    '''\n",
        "    wcs=WebCoverageService(url)\n",
        "    tcc=wcs.contents[service_name]\n",
        "    bbox=tuple(ply.total_bounds)\n",
        "    subsets=[('X',bbox[0],bbox[2]),('Y',bbox[1],bbox[3])]\n",
        "    rsp=wcs.getCoverage(identifier=[tcc.id],subsets=subsets,format='geotiff')\n",
        "    outpath='./'+out_prefix+'.tif'\n",
        "    with open(outpath,'wb') as file:\n",
        "        file.write(rsp.read())\n",
        "\n",
        "    return Raster(outpath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMavd0F0dFaD"
      },
      "source": [
        "##### Step 2: Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGwNO9h7dFaD"
      },
      "outputs": [],
      "source": [
        "# #Get Landsat data from STAC\n",
        "outpath='ls82016.tif'\n",
        "fnd=True\n",
        "if(not os.path.exists(outpath)): #if the 2016 Landsat file does not exits, download it\n",
        "    ls_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
        "    ls_name = \"landsat-c2-l2\"\n",
        "    bnds=['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
        "    ck=get_stac_data(nfp,ls_url,\n",
        "                    name=ls_name,bands=bnds,\n",
        "                    res=30,crs=5070,out_prefix='ls82016',datetime='2016-06-01/2016-06-30', #date time is very important here\n",
        "                    query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},\n",
        "                    limit=1000)\n",
        "    if(ck is None): fnd=False\n",
        "\n",
        "if fnd: ls82016=Raster(outpath)\n",
        "else: \"No objects found in the query\"\n",
        "\n",
        "# #Get 2016 TCC data from Image Service\n",
        "outpath='tcc20161.tif'\n",
        "if(not os.path.exists(outpath)): #if the 2016 tree canopy cover file does not exits, download it\n",
        "    url=r'https://apps.fs.usda.gov/fsgisx01/rest/services/RDW_LandscapeAndWildlife/NLCD_2016_TreeCanopyCover_CONUS/ImageServer'\n",
        "    im_lst=get_image_service_data(url=url,ply=nfp,out_prefix='tcc2016',res=30,outSR=5070)\n",
        "\n",
        "tcc2016=Raster(outpath) #should only be one tile\n",
        "\n",
        "\n",
        "#Get elevation data from USFS 3Dep program web service\n",
        "outpath='./dem.tif' #specify the output path\n",
        "if(not os.path.exists(outpath)): #if the dem file does not exist, download it\n",
        "    geo = nf.envelope.geometry[0] #get the geometry for extent of the study area\n",
        "    d1=py3dep.get_dem(geo,30,4326).expand_dims({'band':1}) # use py3dep to get a xarray surface\n",
        "    d2=Raster(d1).reproject(nfp.crs) #convert d1 into a Raster object\n",
        "    d2.save(outpath) #save it to disk\n",
        "\n",
        "dem=Raster(outpath) #load your dem from disk\n",
        "\n",
        "#Don't we also need Landsat imagery for the year 2021? How can we get that?\n",
        "# outpath='ls82021.tif'\n",
        "# fnd=True\n",
        "# if(not os.path.exists(outpath)): #if the 2016 Landsat file does not exits, download it\n",
        "#     ls_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
        "#     ls_name = \"landsat-c2-l2\"\n",
        "#     bnds=['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
        "#     ck=get_stac_data(nfp,ls_url,\n",
        "#                     name=ls_name,bands=bnds,\n",
        "#                     res=30,crs=5070,out_prefix='ls82016',datetime='2021-06-01/2021-06-30', #date time is very important here\n",
        "#                     query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},\n",
        "#                     limit=1000)\n",
        "#     if(ck is None): fnd=False\n",
        "\n",
        "# if fnd: ls82016=Raster(outpath)\n",
        "# else: \"No objects found in the query\"\n",
        "\n",
        "#TCC for 2021 does not exist on Landfire's image service. How can we get TCC for 2021?\n",
        "# outpath='tcc2021.tif'\n",
        "# if(not os.path.exists(outpath)): #if the 2021 tree canopy cover file does not exits, download it\n",
        "#     url=r'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
        "#     sn='mrlc_download__nlcd_tcc_conus_2021_v2021-4'\n",
        "#     get_wcs_data(url=url,ply=nfp,service_name=sn,out_prefix='tcc2021')\n",
        "\n",
        "# tcc2021=Raster(outpath) #should only be one tile\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT7Ue-vEdFaD"
      },
      "source": [
        "##### Step3: Visualize the Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKQvuEcgdFaD"
      },
      "outputs": [],
      "source": [
        "#Use Raster Tools to display a web map of the first band in the Landsat image\n",
        "display(ls82016.xdata)\n",
        "display(ls82016.explore(band=1,cmap='Reds'))\n",
        "\n",
        "#How can we do the same thing for our tree canopy cover and dem surfaces?\n",
        "# display(tcc2016.xdata)\n",
        "# display(tcc2016.explore(band=1,cmap='Greens'))\n",
        "# display(dem.xdata)\n",
        "# dem.explore(band=1,cmap='terrain')\n",
        "\n",
        "#Can we visualize the Landsat image as a 3 band color composite?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W89xPRrsdFaD"
      },
      "source": [
        "### Section 4: Creating the Data Frame\n",
        "#### In this section we will focus on creating a data frame for further analyses.\n",
        "\n",
        "#### Key learning points:\n",
        "- The utility of data frame\n",
        "- How to use both Raster and Vector data\n",
        "- How to visualize the raw data\n",
        "- How to store the data\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Extract spectral, elevation, and tcc values for each point\n",
        "2. Merge data with points\n",
        "3. Look at the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZK58AZLdFaD"
      },
      "source": [
        "##### Step 1: Extract the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5cRc22KdFaD"
      },
      "outputs": [],
      "source": [
        "lstbl=zonal.extract_points_eager(pnts,ls82016,'ls',axis=1).compute()\n",
        "eltbl=zonal.extract_points_eager(pnts,dem,'el',axis=1).compute()\n",
        "tcctbl=zonal.extract_points_eager(pnts,tcc2016,'tcc',axis=1).compute()\n",
        "\n",
        "#why do we need to compute?\n",
        "\n",
        "#what does axis=1 mean?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjD01K1adFaE"
      },
      "source": [
        "##### Step 2: Merge data with points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vekNS31cdFaE"
      },
      "outputs": [],
      "source": [
        "atr=pd.concat([lstbl,eltbl,tcctbl],axis=1)\n",
        "gdf=gpd.GeoDataFrame(atr,geometry=pnts)\n",
        "\n",
        "#what does axis 1 mean in this context?\n",
        "\n",
        "#Why do we want to use a geodataframe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRbV5SJmdFaE"
      },
      "source": [
        "##### Step 3: Look at the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3B2jBTrdFaE"
      },
      "outputs": [],
      "source": [
        "display(gdf)\n",
        "gdf.explore(column='tcc_1')\n",
        "\n",
        "#How can we save our geodataframe into a format others can use?\n",
        "# gdf.to_file('data.shp.zip') # a\n",
        "# gdf.to_csv('data.csv')\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpzauuhVdFaE"
      },
      "source": [
        "### Section 5: Cleaning Data\n",
        "#### In this section we will explore our data and look for problems that may need to be fixed before we proceed\n",
        "\n",
        "#### Key learning points:\n",
        "- Data are often messy\n",
        "- Missing data can be troublesome to deal with\n",
        "- Ways to identify issues with the data and fix those issues\n",
        "- plotting and graphing\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Finding incorrect values, missing data, and extreme values\n",
        "2. Replacing values\n",
        "3. Dropping records\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvF1vzfzdFaE"
      },
      "source": [
        "##### Step 1: Finding incorrect values or missing data\n",
        "Here we make a distinction between missing data (NA), incorrect values (typos, nodata values), and extreme values. Records with missing data or incorrect values need to be closely examined to see if they need to be removed or replaced, while records with extreme values may highlight unique instances in the data that warrant further investigation. While there are many approaches to cleaning data, here are few common techniques:\n",
        "- Look at the data for obvious errors\n",
        "- Find null values and replace them with the mean of the feature or impute the closest value\n",
        "- Find records with null values and drop those records from the dataset\n",
        "- Use percentiles to identify extreme and potentially incorrect values\n",
        "- Look at correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-LMvjCHdFaE"
      },
      "outputs": [],
      "source": [
        "#Create a map of locations overlaid on raster surfaces\n",
        "import folium\n",
        "m = nfp.explore(name='Boundary')\n",
        "m = gdf.explore(m=m, color='yellow',name='Points')\n",
        "\n",
        "folium.TileLayer(\n",
        "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
        "        attr = 'Esri',\n",
        "        name = 'Esri Imagery',\n",
        "        overlay = False,\n",
        "        control = True\n",
        "       ).add_to(m)\n",
        "\n",
        "\n",
        "# comment and uncomment image layers and rerun\n",
        "m = tcc2016.explore(band=1,map=m,cmap='Greens',name='TCC')\n",
        "#m = ls82016.explore(band=1,map=m,cmap='Reds',name='Landsat Band 1')\n",
        "#m = dem.explore(band=1,map=m,cmap='terrain',name='DEM')\n",
        "\n",
        "m\n",
        "\n",
        "#How can we add all three images to the same map?\n",
        "\n",
        "#Do we see any obvious points we should exclude from our analysis?\n",
        "\n",
        "#What is our population and what does that mean with regards to inference?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0l1LY5UdFaE"
      },
      "outputs": [],
      "source": [
        "#search for null values\n",
        "na_rows=gdf[gdf.isna().any(axis=1)]\n",
        "print('number of rows with na =',na_rows.shape[0])\n",
        "\n",
        "#search for empty values\n",
        "emp_rows=gdf[(gdf=='').any(axis=1)]\n",
        "print('number of rows that have empty values =',emp_rows.shape[0])\n",
        "\n",
        "#what about zero values?\n",
        "zero_rows=gdf[(gdf==0).any(axis=1)]\n",
        "print('number of rows that have zero values =',emp_rows.shape[0])\n",
        "\n",
        "# It does not look like we have any missing data. If we did, how could we remove records or populate missing values?\n",
        "\n",
        "# what about using the feature average?\n",
        "\n",
        "# what about imputation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmhi8h6udFaE"
      },
      "outputs": [],
      "source": [
        "# describe the data\n",
        "print('Summary Statistics')\n",
        "display(atr.describe())\n",
        "\n",
        "# use quantiles to look at extreme values\n",
        "extreme=atr.quantile([0.01,0.99])\n",
        "ex=(atr<extreme.min()).sum(axis=1) + (atr>extreme.max()).sum(axis=1) #sum the number of extreme values in each row\n",
        "gdf['extreme'] = ex #add that attribute to gdf\n",
        "\n",
        "# calculate correlation\n",
        "print('Correlation Matrix')\n",
        "display(atr.corr())\n",
        "\n",
        "# map extreme values\n",
        "print('Map of extreme values (blue to red increased number of extreme values)')\n",
        "display(gdf[ex>0].explore(column='extreme', cmap= 'RdBu_r'))\n",
        "\n",
        "#create a scatter plot matrix of values that highlight the extreme values\n",
        "print('Scatter Plot Matrix (blue to red increased number of extreme values)')\n",
        "display(pd.plotting.scatter_matrix(atr,figsize=(15,15),c=ex,cmap='RdBu_r'))\n",
        "\n",
        "\n",
        "# what relationships do you see?\n",
        "\n",
        "# does there appear to be any observations we should exclude from our analysis?\n",
        "\n",
        "# How can we determine which observations are influential or have leverage?\n",
        "\n",
        "# What if we extracted larger areas than just the cell?\n",
        "\n",
        "# How could we incorporate texture?\n",
        "\n",
        "# what if we created surface derivatives of elevation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarP8c3JdFaE"
      },
      "source": [
        "##### Step 2: Replacing values\n",
        "In this example there were no missing values to replace but what if there were? We will explore 2 options commonly used to replace missing data; 1) using the mean and 2) imputing missing data. For demonstration purposes we will add some missing data to our gdf dataframe and then show how to replace values with the feature's mean value and impute values using [scikit-learn's impute module](https://scikit-learn.org/stable/modules/impute.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr9zKr3idFaE"
      },
      "outputs": [],
      "source": [
        "# Lets make some messy data called mdata using our oud attribute dataframe, extreme values (extreme>5), and a random generator\n",
        "sdt=atr[ex>5]\n",
        "for r in range(sdt.shape[0]):\n",
        "    vls=sdt.iloc[r].copy()\n",
        "    c=np.random.randint(1,7,1)[0]\n",
        "    vls.iloc[c]=np.nan\n",
        "    sdt.iloc[r]=vls\n",
        "\n",
        "sdt\n",
        "mdata=atr.copy()\n",
        "mdata[ex>5]=sdt\n",
        "\n",
        "print('Number of extreme values = ',(ex>5).sum())\n",
        "print('Number of nans =',mdata.isna().sum().sum())\n",
        "\n",
        "#show the messy data\n",
        "print('Here are the rows with nans...')\n",
        "display(mdata[ex>5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqbeUBTkdFaE"
      },
      "outputs": [],
      "source": [
        "#let's us scikit-learn's SimpleImputer to fill in the mean value for a nans\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean') #Specify the SimpleImputer (mean)\n",
        "imp.fit(mdata) #fit the mdata for all non nan values\n",
        "cdata=pd.DataFrame(imp.transform(mdata),columns=mdata.columns) #create the new data frame with no nans\n",
        "\n",
        "#look at the changed records\n",
        "cdata[ex>5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqNNfe_NdFaF"
      },
      "outputs": [],
      "source": [
        "#let's us scikit-learn's IterativeImputer to fill in the nan values\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "imp = IterativeImputer(max_iter=10, random_state=0)\n",
        "imp.fit(mdata)\n",
        "cdata2=pd.DataFrame(imp.transform(mdata),columns=mdata.columns) #create the new data frame with no nans\n",
        "\n",
        "#look at the changed records\n",
        "cdata2[ex>5]\n",
        "\n",
        "#why are the values different?\n",
        "\n",
        "#which technique is better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7eD4AXMdFaF"
      },
      "source": [
        "##### Step 3: Dropping records\n",
        "What if so much of the data for a record is gone or is somehow incorrect that it really does not add anything to our analysis or worse yet detracts from our analysis? In that case we may want to think about removing that record from our dataset. While this approach should be used as a last resort, it is very easy to implement. For demonstration purposes, we will show how to select records and remove them from our geodataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpY2FMxfdFaF"
      },
      "outputs": [],
      "source": [
        "#let's drop all records with a nan value\n",
        "cdata3=mdata.dropna()\n",
        "\n",
        "print('The number of rows in cdata4 =',cdata3.shape[0])\n",
        "\n",
        "#why do we need to remove na values?\n",
        "\n",
        "#why is droping rows used as last resort?\n",
        "\n",
        "#How can you change a know value that is incorrect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxkzN2ldFaF"
      },
      "source": [
        "#### Preprocessing\n",
        "##### In this section we will begin looking at ways to address trending in our data and potentially reducing dimensionality.\n",
        "\n",
        "##### Key learning points:\n",
        "- The curse of dimensionality\n",
        "- Transformations\n",
        "- Ordination\n",
        "- Deciding on predictors\n",
        "- Using plotting and graphing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKVSiigPdFaF"
      },
      "source": [
        "#### Building a predictive model\n",
        "##### In this section we will explore how to build a model that can be used later to predict new estimates\n",
        "\n",
        "##### Key learning points:\n",
        "- modeling assumptions\n",
        "- Bias\n",
        "- training, testing, and validation\n",
        "- Model estimates and error\n",
        "- Estimation domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBQ3ZycSdFaF"
      },
      "source": [
        "#### Estimating % Tree Canopy Cover\n",
        "##### In this section we will explore how to use a model to create new estimates and surfaces\n",
        "\n",
        "##### Key learning points:\n",
        "- Estimates may not reflect reality\n",
        "- The power of more estimates\n",
        "- Aggregating values\n",
        "- Estimates of error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETQD5ejZdFaF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}