{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jshogland/SpatialModelingTutorials/blob/main/Notebooks/ShawneeNF_TCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv69joTwdFaA"
      },
      "source": [
        "# Percent Tree Canopy Cover Notebook\n",
        "## In this notebook we will explore how to build a simple random sample of point locations, extract remotely sensed data for those locations, and use that data to estimate % Tree Cover for the Shawnee National Forest.\n",
        "\n",
        "### Project objective:\n",
        "- Build a machine learning model from estimates of 2016 percent forest canopy cover, elevation data, and Landsat 8 imagery for that same time period\n",
        "- Apply that model to elevation and Landsat 8 data acquired in 2021 to estimate percent forest canopy cover in 2021\n",
        "- Compare our model estimates to [MLRC](https://www.mrlc.gov/data-services-page) % forest canopy cover estimates for the year 2021\n",
        "\n",
        "#### Notebook Sections:\n",
        "1. Setup\n",
        "2. Create a Sample\n",
        "3. Download data\n",
        "4. Creating the Data Frame\n",
        "5. Clean data\n",
        "6. Preprocessing\n",
        "7. Build a predictive model\n",
        "8. Estimate Percent Tree Canopy Cover\n",
        "\n",
        "#### Data sources\n",
        "- [Landsat 8](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2) - Extracted from Planetary Computer ([STAC](https://stacspec.org/en))\n",
        "- [Tree Canopy Cover 2016](https://apps.fs.usda.gov/fsgisx01/rest/services/RDW_LandscapeAndWildlife/NLCD_2016_TreeCanopyCover_CONUS/ImageServer) - Extracted from Landfire Image Service ([REST](https://developers.arcgis.com/rest/services-reference/enterprise/image-service/))\n",
        "- [USGS Elevation](https://www.usgs.gov/3d-elevation-program) - Extracted from USGS 3DEP program ([py3dep](https://github.com/hyriver/py3dep))\n",
        "- [National Forest Boundary](https://www.openstreetmap.org/#map=9/34.306/-112.242) - Extracted from Open Street Maps ([osmnx](https://osmnx.readthedocs.io/en/stable/))\n",
        "\n",
        "Author John Hogland 10/1/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdouEjUmdFaB"
      },
      "source": [
        "### Section 1: Setup\n",
        "#### Installing software for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdqdQ1_PdFaB"
      },
      "outputs": [],
      "source": [
        "!pip install mapclassify\n",
        "!pip install osmnx\n",
        "!pip install raster_tools\n",
        "!pip install planetary-computer\n",
        "!pip install pystac-client\n",
        "!pip install stackstac\n",
        "!pip install py3dep==0.17.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIuHENindFaB"
      },
      "source": [
        "#### Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTkFqp2LdFaC"
      },
      "outputs": [],
      "source": [
        "#get packages\n",
        "import osmnx as ox, planetary_computer, pystac_client, stackstac\n",
        "import geopandas as gpd, pandas as pd, os, numpy as np, requests, urllib, py3dep\n",
        "\n",
        "from raster_tools import Raster,zonal, general, Vector\n",
        "from shapely import geometry\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGSrvI_VdFaC",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### Section 2: Create a sample\n",
        "#### In this section we will extract the spatial boundary of the Shawnee National Forest using Open Street Maps (OSM) Web Service and create a simple random sample of locations within the boundary of the National forest.\n",
        "\n",
        "#### Key learning points:\n",
        "- Using OSM to get vector data (downloading)\n",
        "- Generating a simple random sample of point location that will be used to create a data frame of response and predictor variables\n",
        "- How to Visualize the the boundary and point locations\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Use osmnx and OSM to get a GeoDataFrame (polygons) of the boundary of the Shawnee National Forest\n",
        "2. Use the Shawnee National Forest boundary and Geopanda's sample_points function to create a simple random sample of point locations\n",
        "3. Create a interactive map that displays our point locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJhsSKWPdFaC"
      },
      "source": [
        "##### Step 1: Get the Shawnee National Forest boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-OrUepEdFaC"
      },
      "outputs": [],
      "source": [
        "#use osmnx OpenStreetMaps to get the boundary of the NF (GeoDataFrame)\n",
        "nf=ox.geocode_to_gdf('Shawnee National Forest, IL, USA')\n",
        "\n",
        "#Look at the coordinate system of the GeoDataFrame\n",
        "print('Original CRS =',nf.crs)\n",
        "\n",
        "#project the GeoDataFrame to Albers equal area EPSG:5070\n",
        "nfp=nf.to_crs(5070)\n",
        "\n",
        "#Look at the GeoDataFrame\n",
        "display(nfp)\n",
        "\n",
        "#Plot the national forest\n",
        "nfp.plot(figsize=(15,15))\n",
        "\n",
        "#Why are we projecting to Albers equal Area?\n",
        "\n",
        "#How many acres is the National Forest?\n",
        "print('Number of acres =',(nfp.area * 0.000247105).values[0])\n",
        "\n",
        "#How many polygons are in the National Forest?\n",
        "print('Number of multipolygons =',nfp.shape[0])\n",
        "print('Number of polygons =',nfp.explode().shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SmIuf2dFaC"
      },
      "source": [
        "##### Step 3: Create the simple random sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GT6iqyxdFaC"
      },
      "outputs": [],
      "source": [
        "#us random sample function to create 2,000 locations within the nfp\n",
        "mpnts=nfp.sample_points(2000)\n",
        "\n",
        "#Look at the GeoSeries\n",
        "mpnts\n",
        "\n",
        "#Why 2,000 locations? Why not 100 or 10,000?\n",
        "\n",
        "#How many records do we have?\n",
        "#print('Number of records =',mpnts.shape[0])\n",
        "\n",
        "#How many observations do we have?\n",
        "#print('Number of points (observations) =',mpnts.explode().shape[0])\n",
        "\n",
        "#How can we convert our multipoint to points?\n",
        "pnts=mpnts.explode().reset_index(drop=True) #reseting the index to clean things up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elx0kt__dFaC"
      },
      "source": [
        "##### Step 4: Visualize the national forest boundary and point locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHmkhbhXdFaD"
      },
      "outputs": [],
      "source": [
        "#Use geopandas explore function to create a interactive map\n",
        "m=nfp.explore(color='blue') #create the map using the National Forest Boundary\n",
        "m=pnts.explore(m=m,color='yellow') #add out points to the map\n",
        "m #show the map\n",
        "\n",
        "#Can we save our map to share with others?\n",
        "#m.save('sample.html')\n",
        "\n",
        "#How can we plot our map?\n",
        "# p=nfp.plot(edgecolor='blue',facecolor='none',figsize=(15,15))\n",
        "# p=pnts.plot(ax=p,color='yellow')\n",
        "# p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-9HKYGdFaD"
      },
      "source": [
        "### Section 3: Downloading Raster Data\n",
        "#### In this section we will focus on download raster dataset from various sources.\n",
        "\n",
        "#### Key learning points:\n",
        "- Accessing data from different web services\n",
        "- Deciding where to do your processing (client or server side)\n",
        "- How and when to store the data locally\n",
        "- How to visualizing the raster surfaces\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Create functions to download data from STAC, REST, WCS\n",
        "2. Use those functions to create Raster datasets\n",
        "3. Visualize Your Raster datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4qM4HJdFaD"
      },
      "source": [
        "##### Step 1: Creating download definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMe0XcOadFaD"
      },
      "outputs": [],
      "source": [
        "from owslib.wcs import WebCoverageService\n",
        "#create definition to mosaic stac data\n",
        "def mosaic_stac(xr):\n",
        "    '''\n",
        "    Creates a mosaic from multi-temporal xarray stack\n",
        "\n",
        "    xr=Xarray object\n",
        "\n",
        "    returns xarray mosaic\n",
        "    '''\n",
        "    return stackstac.mosaic(xr)\n",
        "\n",
        "#create definition to extract stac data\n",
        "def get_stac_data(ply,url,name,bands,res=30,crs=5070,\n",
        "                  out_prefix='ls8',**kwarg):\n",
        "    '''\n",
        "    gets tiled data from planetary computer as a dask backed xarray that intersects the geometry of the point, line, or polygon\n",
        "\n",
        "    ply = (geoseries or geodataframe) of the study area\n",
        "    url = (string) base url to planetary computer https://planetarycomputer.microsoft.com/api/stac/v1\n",
        "    name = (string) catelog resource e.g., \"sentinel-2-l2a\"\n",
        "    bands = (list(string)) of attributes ['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
        "    qry =  (dictionary) of property values {'eo:cloud_cover':{'lt':1}}\n",
        "    res = (tuple of numbers) output resolution (x,y)\n",
        "    crs = (int) output crs\n",
        "    dt = (string) data time intervale e.g., one month: 2023-06, range: 2023-06-02/2023-06-17\n",
        "    limit = (int) max number of items to return\n",
        "    out_prefix = (string) prefix used to save the image\n",
        "\n",
        "    returns a Raster object\n",
        "    '''\n",
        "    geo = ply.to_crs('4326').envelope.geometry[0]\n",
        "    xmin,ymin,xmax,ymax=ply.total_bounds\n",
        "    catalog = pystac_client.Client.open(url, modifier=planetary_computer.sign_inplace)\n",
        "    srch = catalog.search(collections=name, intersects=geo, **kwarg)\n",
        "    ic = srch.item_collection()\n",
        "    if(len(ic.items)>0):\n",
        "        xra = stackstac.stack(ic,resolution=res,epsg=crs)\n",
        "        xra = mosaic_stac(xra)\n",
        "        rs=Raster(xra.sel(band=bands,x=slice(xmin,xmax),y=slice(ymax,ymin)))\n",
        "        outpath=out_prefix+'.tif'\n",
        "        rs.save(outpath)\n",
        "        rs=Raster(outpath)\n",
        "    else:\n",
        "        rs=None\n",
        "\n",
        "    return rs\n",
        "\n",
        "#Create definition to extract image service data\n",
        "def get_image_service_data(url, ply, out_prefix,res=30,outSR=\"\"):\n",
        "    '''\n",
        "    extracts a list of images from a image service given a url, polygon, and output prefix name\n",
        "\n",
        "    url = (string) path to image service e.g., url=r'https://lfps.usgs.gov/arcgis/rest/services/Landfire_LF230/US_230EVT/ImageServer'\n",
        "    ply = (geoseries or geodataframe) of the study area\n",
        "    out_prefix = (string) prefix used to save each image\n",
        "\n",
        "    returns a list of Raster objects, one for each tile\n",
        "    '''\n",
        "    layerInfo=requests.get(url+'?f=pjson')\n",
        "    dic=layerInfo.json()\n",
        "    #print(dic)\n",
        "    spr=dic['spatialReference']\n",
        "    m_width=dic['maxImageWidth']\n",
        "    m_height=dic['maxImageHeight']\n",
        "    fitem=next(iter(spr))\n",
        "    ply2=ply.to_crs(spr[fitem])\n",
        "\n",
        "    xmin,ymin,xmax,ymax=ply2.total_bounds\n",
        "\n",
        "    wcells=int((xmax-xmin)/res)\n",
        "    hcells=int((ymax-ymin)/res)\n",
        "\n",
        "    if(wcells<m_width):\n",
        "        m_width=wcells\n",
        "\n",
        "    if(hcells<m_height):\n",
        "        m_height=hcells\n",
        "\n",
        "\n",
        "    wcells_l=np.arange(0,wcells,m_width)\n",
        "    hcells_l=np.arange(0,hcells,m_height)\n",
        "\n",
        "    xmax2=xmin\n",
        "    ymax2=ymin\n",
        "\n",
        "    tile=1\n",
        "\n",
        "    rs_lst=[]\n",
        "    for w in wcells_l:\n",
        "        for h in hcells_l:\n",
        "            xmax2 = (m_width*res+xmax2)\n",
        "            ymax2 = (m_height*res+ymax2)\n",
        "\n",
        "            qry = url+'/exportImage?'\n",
        "            parm = {\n",
        "                'f':'json',\n",
        "                'bbox':','.join([str(xmin),str(ymin),str(xmax2),str(ymax2)]),\n",
        "                'size':str(m_width) + ',' + str(m_height),\n",
        "                'imageSR':outSR,\n",
        "                'format':'tiff'\n",
        "            }\n",
        "            #print(parm['bbox'])\n",
        "            response=requests.get(qry,parm)\n",
        "            if response.status_code == 200:\n",
        "                img_url=response.json()['href']\n",
        "                outname=out_prefix + str(tile) + '.tif'\n",
        "                urllib.request.urlretrieve(img_url, outname)\n",
        "                rs_lst.append(Raster(outname))\n",
        "                tile+=1\n",
        "\n",
        "    return rs_lst\n",
        "\n",
        "# Creat definition for WCS download\n",
        "def get_wcs_data(url,ply,service_name='mrlc_download__nlcd_tcc_conus_2021_v2021-4',out_prefix = 'tcc'):\n",
        "    '''\n",
        "    Extracts saves an image from a WCS given url, polygon boundary, and service name. Images are saved in the same location as the notebook.\n",
        "    url = (string) path to wcs e.g. 'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
        "    ply = (geoseries or geodataframe) of the study area\n",
        "    service_name = (string) name of the service e.g. mrlc_download__nlcd_tcc_conus_2021_v2021-4\n",
        "    out_prefix = (string) prefix used to save each image\n",
        "\n",
        "    returns a Raster object\n",
        "    '''\n",
        "    wcs=WebCoverageService(url)\n",
        "    tcc=wcs.contents[service_name]\n",
        "    bbox=tuple(ply.total_bounds)\n",
        "    subsets=[('X',bbox[0],bbox[2]),('Y',bbox[1],bbox[3])]\n",
        "    rsp=wcs.getCoverage(identifier=[tcc.id],subsets=subsets,format='geotiff')\n",
        "    outpath='./'+out_prefix+'.tif'\n",
        "    with open(outpath,'wb') as file:\n",
        "        file.write(rsp.read())\n",
        "\n",
        "    return Raster(outpath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMavd0F0dFaD"
      },
      "source": [
        "##### Step 2: Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGwNO9h7dFaD"
      },
      "outputs": [],
      "source": [
        "# #Get Landsat data from STAC\n",
        "outpath='ls82016.tif'\n",
        "fnd=True\n",
        "if(not os.path.exists(outpath)): #if the 2016 Landsat file does not exits, download it\n",
        "    ls_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
        "    ls_name = \"landsat-c2-l2\"\n",
        "    bnds=['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
        "    ck=get_stac_data(nfp,ls_url,\n",
        "                    name=ls_name,bands=bnds,\n",
        "                    res=30,crs=5070,out_prefix='ls82016',datetime='2016-06-01/2016-06-30', #date time is very important here\n",
        "                    query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},\n",
        "                    limit=1000)\n",
        "    if(ck is None): fnd=False\n",
        "\n",
        "if fnd: ls82016=Raster(outpath)\n",
        "else: \"No objects found in the query\"\n",
        "\n",
        "# #Get 2016 TCC data from Image Service\n",
        "outpath='tcc20161.tif'\n",
        "if(not os.path.exists(outpath)): #if the 2016 tree canopy cover file does not exits, download it\n",
        "    url=r'https://apps.fs.usda.gov/fsgisx01/rest/services/RDW_LandscapeAndWildlife/NLCD_2016_TreeCanopyCover_CONUS/ImageServer'\n",
        "    im_lst=get_image_service_data(url=url,ply=nfp,out_prefix='tcc2016',res=30,outSR=5070)\n",
        "\n",
        "tcc2016=Raster(outpath) #should only be one tile\n",
        "\n",
        "\n",
        "#Get elevation data from USFS 3Dep program web service\n",
        "outpath='./dem.tif' #specify the output path\n",
        "if(not os.path.exists(outpath)): #if the dem file does not exist, download it\n",
        "    geo = nf.envelope.geometry[0] #get the geometry for extent of the study area\n",
        "    d1=py3dep.get_dem(geo,30,4326).expand_dims({'band':1}) # use py3dep to get a xarray surface\n",
        "    d2=Raster(d1).reproject(nfp.crs) #convert d1 into a Raster object\n",
        "    d2.save(outpath) #save it to disk\n",
        "\n",
        "dem=Raster(outpath) #load your dem from disk\n",
        "\n",
        "#Don't we also need Landsat imagery for the year 2021? How can we get that?\n",
        "# outpath='ls82021.tif'\n",
        "# fnd=True\n",
        "# if(not os.path.exists(outpath)): #if the 2016 Landsat file does not exits, download it\n",
        "#     ls_url = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
        "#     ls_name = \"landsat-c2-l2\"\n",
        "#     bnds=['red', 'blue', 'green', 'nir08', 'lwir11','swir16', 'swir22']\n",
        "#     ck=get_stac_data(nfp,ls_url,\n",
        "#                     name=ls_name,bands=bnds,\n",
        "#                     res=30,crs=5070,out_prefix='ls82016',datetime='2021-06-01/2021-06-30', #date time is very important here\n",
        "#                     query={'eo:cloud_cover':{'lt':10},'platform':{'eq':'landsat-8'}},\n",
        "#                     limit=1000)\n",
        "#     if(ck is None): fnd=False\n",
        "\n",
        "# if fnd: ls82016=Raster(outpath)\n",
        "# else: \"No objects found in the query\"\n",
        "\n",
        "#TCC for 2021 does not exist on Landfire's image service. How can we get TCC for 2021?\n",
        "# outpath='tcc2021.tif'\n",
        "# if(not os.path.exists(outpath)): #if the 2021 tree canopy cover file does not exits, download it\n",
        "#     url=r'https://www.mrlc.gov/geoserver/mrlc_download/nlcd_tcc_conus_2021_v2021-4/wcs?'\n",
        "#     sn='mrlc_download__nlcd_tcc_conus_2021_v2021-4'\n",
        "#     get_wcs_data(url=url,ply=nfp,service_name=sn,out_prefix='tcc2021')\n",
        "\n",
        "# tcc2021=Raster(outpath) #should only be one tile\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT7Ue-vEdFaD"
      },
      "source": [
        "##### Step3: Visualize the Rasters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKQvuEcgdFaD"
      },
      "outputs": [],
      "source": [
        "#Use Raster Tools to display a web map of the first band in the Landsat image\n",
        "display(ls82016.xdata)\n",
        "display(ls82016.explore(band=1,cmap='Reds'))\n",
        "\n",
        "#How can we do the same thing for our tree canopy cover and dem surfaces?\n",
        "# display(tcc2016.xdata)\n",
        "# display(tcc2016.explore(band=1,cmap='Greens'))\n",
        "# display(dem.xdata)\n",
        "# dem.explore(band=1,cmap='terrain')\n",
        "\n",
        "#Can we visualize the Landsat image as a 3 band color composite?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W89xPRrsdFaD"
      },
      "source": [
        "### Section 4: Creating the Data Frame\n",
        "#### In this section we will focus on creating a data frame for further analyses.\n",
        "\n",
        "#### Key learning points:\n",
        "- The utility of data frame\n",
        "- How to use both Raster and Vector data\n",
        "- How to visualize the raw data\n",
        "- How to store the data\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Extract spectral, elevation, and tcc values for each point\n",
        "2. Merge data with points\n",
        "3. Look at the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZK58AZLdFaD"
      },
      "source": [
        "##### Step 1: Extract the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5cRc22KdFaD"
      },
      "outputs": [],
      "source": [
        "lstbl=zonal.extract_points_eager(pnts,ls82016,'ls',axis=1).compute()\n",
        "eltbl=zonal.extract_points_eager(pnts,dem,'el',axis=1).compute()\n",
        "tcctbl=zonal.extract_points_eager(pnts,tcc2016,'tcc',axis=1).compute()\n",
        "\n",
        "#why do we need to compute?\n",
        "\n",
        "#what does axis=1 mean?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjD01K1adFaE"
      },
      "source": [
        "##### Step 2: Merge data with points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vekNS31cdFaE"
      },
      "outputs": [],
      "source": [
        "atr=pd.concat([lstbl,eltbl,tcctbl],axis=1)\n",
        "gdf=gpd.GeoDataFrame(atr,geometry=pnts)\n",
        "\n",
        "#what does axis 1 mean in this context?\n",
        "\n",
        "#Why do we want to use a geodataframe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRbV5SJmdFaE"
      },
      "source": [
        "##### Step 3: Look at the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3B2jBTrdFaE"
      },
      "outputs": [],
      "source": [
        "display(gdf)\n",
        "gdf.explore(column='tcc_1')\n",
        "\n",
        "#How can we save our geodataframe into a format others can use?\n",
        "# gdf.to_file('data.shp.zip') # a\n",
        "# gdf.to_csv('data.csv')\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpzauuhVdFaE"
      },
      "source": [
        "### Section 5: Cleaning Data\n",
        "#### In this section we will explore our data and look for problems that may need to be fixed before we proceed\n",
        "\n",
        "#### Key learning points:\n",
        "- Data are often messy\n",
        "- Missing data can be troublesome to deal with\n",
        "- Ways to identify issues with the data and fix those issues\n",
        "- plotting and graphing\n",
        "\n",
        "#### Coding Steps:\n",
        "1. Finding incorrect values, missing data, and extreme values\n",
        "2. Replacing values\n",
        "3. Dropping records\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvF1vzfzdFaE"
      },
      "source": [
        "##### Step 1: Finding incorrect values or missing data\n",
        "Here we make a distinction between missing data (NA), incorrect values (typos, nodata values), and extreme values. Records with missing data or incorrect values need to be closely examined to see if they need to be removed or replaced, while records with extreme values may highlight unique instances in the data that warrant further investigation. While there are many approaches to cleaning data, here are few common techniques:\n",
        "- Look at the data for obvious errors\n",
        "- Find null values and replace them with the mean of the feature or impute the closest value\n",
        "- Find records with null values and drop those records from the dataset\n",
        "- Use percentiles to identify extreme and potentially incorrect values\n",
        "- Look at correlation\n",
        "\n",
        "Looking for obvious errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-LMvjCHdFaE"
      },
      "outputs": [],
      "source": [
        "#Create a map of locations overlaid on raster surfaces\n",
        "import folium\n",
        "m = nfp.explore(name='Boundary')\n",
        "m = gdf.explore(m=m, color='yellow',name='Points')\n",
        "\n",
        "folium.TileLayer(\n",
        "        tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
        "        attr = 'Esri',\n",
        "        name = 'Esri Imagery',\n",
        "        overlay = False,\n",
        "        control = True\n",
        "       ).add_to(m)\n",
        "\n",
        "\n",
        "# comment and uncomment image layers and rerun\n",
        "m = tcc2016.explore(band=1,map=m,cmap='Greens',name='TCC')\n",
        "#m = ls82016.explore(band=1,map=m,cmap='Reds',name='Landsat Band 1')\n",
        "#m = dem.explore(band=1,map=m,cmap='terrain',name='DEM')\n",
        "\n",
        "m\n",
        "\n",
        "#How can we add all three images to the same map?\n",
        "\n",
        "#Do we see any obvious points we should exclude from our analysis?\n",
        "\n",
        "#What is our population and what does that mean with regards to inference?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking for null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0l1LY5UdFaE"
      },
      "outputs": [],
      "source": [
        "#search for null values\n",
        "na_rows=gdf[gdf.isna().any(axis=1)]\n",
        "print('number of rows with na =',na_rows.shape[0])\n",
        "\n",
        "#search for empty values\n",
        "emp_rows=gdf[(gdf=='').any(axis=1)]\n",
        "print('number of rows that have empty values =',emp_rows.shape[0])\n",
        "\n",
        "#what about zero values?\n",
        "zero_rows=gdf[(gdf==0).any(axis=1)]\n",
        "print('number of rows that have zero values =',emp_rows.shape[0])\n",
        "\n",
        "# It does not look like we have any missing data. If we did, how could we remove records or populate missing values?\n",
        "\n",
        "# what about using the feature average?\n",
        "\n",
        "# what about imputation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describing the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmhi8h6udFaE"
      },
      "outputs": [],
      "source": [
        "#look at summary statistics for continuous data\n",
        "print('Summary stats for continuous variables...')\n",
        "display(gdf.describe())\n",
        "\n",
        "# use quantiles to look at extreme values (continuous)\n",
        "extreme=atr.quantile([0.01,0.99])\n",
        "ex=(atr<extreme.min()).sum(axis=1) + (atr>extreme.max()).sum(axis=1) #sum the number of extreme values in each row\n",
        "gdf['extreme'] = ex #add that attribute to gdf\n",
        "\n",
        "# calculate correlation\n",
        "print('','Correlation Matrix')\n",
        "display(atr.corr())\n",
        "\n",
        "# map extreme values\n",
        "print('Map of extreme values (blue to red increased number of extreme values)')\n",
        "display(gdf[ex>0].explore(column='extreme', cmap= 'RdBu_r'))\n",
        "\n",
        "#create a scatter plot matrix of values that highlight the extreme values\n",
        "print('Scatter Plot Matrix (blue to red increased number of extreme values)')\n",
        "display(pd.plotting.scatter_matrix(atr,figsize=(15,15),c=ex,cmap='RdBu_r'))\n",
        "\n",
        "\n",
        "# what relationships do you see?\n",
        "\n",
        "# does there appear to be any observations we should exclude from our analysis?\n",
        "\n",
        "# How can we determine which observations are influential or have leverage?\n",
        "\n",
        "# What if we extracted larger areas than just the cell?\n",
        "\n",
        "# How could we incorporate texture?\n",
        "\n",
        "# what if we created surface derivatives of elevation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example all of our data is continuous and clean. While this is ideal, it would be a good idea to address messy data issues. To do this let's make some categorical data, add a little random noise to that data, and insert null values. For our new categorical variables, one will be a response variable called forest_density with labeling errors and the other variable will be a predictor variable called red_cat with data recording errors. Finally, for our continuous predictor variables we will randomly add in null values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First let's copy our dataframe and make a new dataframe called gdf_m\n",
        "gdf_m = gdf.copy()\n",
        "\n",
        "# Let's create forest_density with the categories Low, medium, and high based on tcc_1\n",
        "tcc=gdf_m['tcc_1']\n",
        "l=tcc<20\n",
        "h=tcc>80\n",
        "m=(20<=tcc) & (tcc<=80)\n",
        "gdf_m.loc[l,'forest_density']='low'\n",
        "gdf_m.loc[m,'forest_density']='medium'\n",
        "gdf_m.loc[h,'forest_density']='high'\n",
        "\n",
        "# Let's create red_cat with categories 1=dark, 2=normal, and 3=bright based on ls_1\n",
        "ls_1=gdf_m['ls_1']\n",
        "d=ls_1<0.022\n",
        "b=ls_1>0.024\n",
        "n=(0.020<=ls_1) & (ls_1<=0.024)\n",
        "gdf_m.loc[l,'red_cat']=1\n",
        "gdf_m.loc[m,'red_cat']=2\n",
        "gdf_m.loc[h,'red_cat']=3\n",
        "\n",
        "# Let's make some messy data for forest_density (labeling error for the first 1000 records)\n",
        "fd=gdf_m['forest_density'].copy()\n",
        "fd.iloc[:1000]=fd[:1000].astype('str').str[0]\n",
        "gdf_m['forest_density']=fd\n",
        "\n",
        "# Let's make some messy data for red_cat (data recorder error, value = 0)\n",
        "rvl=gdf_m['red_cat'].copy()\n",
        "rvl.iloc[np.random.choice(np.arange(2000),10,replace=False)]=0\n",
        "gdf_m['red_cat']=rvl.astype('str')\n",
        "\n",
        "# Look at the data\n",
        "display(gdf_m)\n",
        "#look at summary statistics for categorical data\n",
        "print('Summary stats for categorical variables')\n",
        "display(gdf_m.describe(include='object'))\n",
        "\n",
        "#look at the unique categorical values\n",
        "print('Unique values and counts for forest_density')\n",
        "print(np.unique(gdf_m['forest_density'],return_counts=True))\n",
        "\n",
        "print('Unique values and counts for red_cat')\n",
        "print(np.unique(gdf_m['red_cat'],return_counts=True))\n",
        "\n",
        "# Lets make some messy data for randomly chosen continuous predictor variables that have extreme values (extreme>5)\n",
        "sdt=gdf_m[ex>5].copy()\n",
        "for r in range(sdt.shape[0]):\n",
        "    vls=sdt.iloc[r].copy()\n",
        "    c=np.random.randint(1,7,1)[0]\n",
        "    vls.iloc[c]=np.nan\n",
        "    sdt.iloc[r]=vls\n",
        "\n",
        "gdf_m.loc[ex>5]=sdt\n",
        "\n",
        "print('\\nNumber of extreme values = ',(ex>5).sum())\n",
        "print('Number of nans =',gdf_m.isna().sum().sum())\n",
        "\n",
        "#show the messy data\n",
        "print('\\nHere are the rows with nans...')\n",
        "display(gdf_m[ex>5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarP8c3JdFaE"
      },
      "source": [
        "##### Step 2: Replacing values\n",
        "Now that we have some messy data, we can explore some commonly used ways to replace missing data; 1) using univariate statistics and 2) imputing missing data. To do this we will be using [scikit-learn's impute module](https://scikit-learn.org/stable/modules/impute.html).\n",
        "\n",
        "Let's start with the labeling errors in forest_density."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we need our labels to be consistent. To fix this issue let's use a mapping dictionary\n",
        "rdic={\n",
        "    'l':'low',\n",
        "    'm':'medium',\n",
        "    'h':'high'\n",
        "}\n",
        "gdf_m['forest_density']=gdf_m['forest_density'].replace(rdic) #update the forest_density column\n",
        "\n",
        "#let's look at the change\n",
        "gdf_m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the easy ones taken care of, let's make 2 more copies of the data to explore different ways of addressing missing data (Simple Imputer, Iterative Imputer, and Dropping records).\n",
        "\n",
        "First let's fix the null values for continuous predictor variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqbeUBTkdFaE"
      },
      "outputs": [],
      "source": [
        "#make 3 more copies of gdf_m. \n",
        "# gdf_m is used as the original, \n",
        "# gdf_m1 is used for Simple Imputer,\n",
        "# gdf_m2 is used for Iterative Imputer\n",
        "# gdf_m3 is used for dropping records\n",
        "gdf_m1=gdf_m.copy() \n",
        "gdf_m2=gdf_m.copy() \n",
        "gdf_m3=gdf_m.copy()\n",
        "\n",
        "#let's us scikit-learn's SimpleImputer to fill in the mean value for a nans\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#subset the data to the columns that have null or nan values\n",
        "mdata=gdf_m1[gdf_m1.columns[:7]]\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean') #Specify the SimpleImputer (mean with missing value = nan)\n",
        "imp.fit(mdata) #fit the mdata for all non nan values\n",
        "cdata=pd.DataFrame(imp.transform(mdata),columns=gdf_m1.columns[:7]) #create the new data frame with no nans\n",
        "\n",
        "#look at the changed records\n",
        "display(cdata[ex>5])\n",
        "\n",
        "#update the records in gdf_m\n",
        "gdf_m1[gdf_m1.columns[:7]]=cdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's tackle the data recording errors in red_cat ('0.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rc=gdf_m1[['red_cat']] #setup the dataframe for scikit learn\n",
        "imp2=SimpleImputer(missing_values='0.0', strategy='most_frequent') #Specify the SimpleImputer (most_frequent with missing value= '0')\n",
        "imp2.fit(rc) #fit the rc data for all '0'\n",
        "cdata2=pd.DataFrame(imp2.transform(rc),columns=['red_cat']) #create the new data frame with no '0'\n",
        "\n",
        "#display the records that had 0.0 and look at their new values\n",
        "display(cdata2.loc[gdf_m['red_cat']=='0.0'])\n",
        "\n",
        "#update gdf_m red_cat values\n",
        "gdf_m1['red_cat']=cdata2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's do the same thing with the Iterative Imputer and our continuous variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqNNfe_NdFaF"
      },
      "outputs": [],
      "source": [
        "#let's us scikit-learn's IterativeImputer to fill in the nan values\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "mdata=gdf_m2[gdf_m2.columns[:7]]\n",
        "imp = IterativeImputer(max_iter=10, random_state=0)\n",
        "imp.fit(mdata)\n",
        "cdata2=pd.DataFrame(imp.transform(mdata),columns=mdata.columns) #create the new data frame with no nans\n",
        "\n",
        "#look at the changed records\n",
        "display(cdata2[ex>5])\n",
        "\n",
        "#update the records in gdf_m2\n",
        "gdf_m2[gdf_m2.columns[:7]]=cdata\n",
        "\n",
        "#why are the values different?\n",
        "\n",
        "#which technique is better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Don't forget red_cat. We have many options here including using the IterativeImputer but let's use a RandomForest Classifier "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# First split out our data based on the records that have '0.0' value in the red_cat and that have a real label (1-3)\n",
        "categorical = 'red_cat' #identify the categorical response variable name\n",
        "pred = gdf_m2.columns[:7] #identify the predictor variables name\n",
        "\n",
        "clms=list(pred) + [categorical] #get all variable names used in the analysis\n",
        "mdata=gdf_m2[clms] #create a intermediate dataset\n",
        "\n",
        "#split the data into training data and new data we want to predict\n",
        "ndata=mdata[mdata[categorical]=='0.0'][pred] #these are the records we want to predict a label \n",
        "if(ndata.shape[0]>0): #do a quick look to see if values have alread been imputed\n",
        "    train=mdata[clms][~mdata.index.isin(ndata.index)] #these are the records we will use to train our model\n",
        "    X=train[pred] #predictor variables and values\n",
        "    y=train[categorical] #response variable and values\n",
        "\n",
        "    rf=RandomForestClassifier() #create the randomforest classifier\n",
        "    rf.fit(X,y) #fit the model using the training data\n",
        "    lbl=rf.predict(ndata) #predict the labels for the new data (records with '0.0' )\n",
        "    display(lbl) #look at the predictions\n",
        "\n",
        "    #update the database with the new labels\n",
        "    gdf_m2.loc[gdf_m2[categorical]=='0.0',categorical]=lbl\n",
        "\n",
        "#look at the changed records\n",
        "gdf_m2.loc[gdf_m[categorical]=='0.0']\n",
        "\n",
        "#How is this different than the IterativeImputer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7eD4AXMdFaF"
      },
      "source": [
        "##### Step 3: Dropping records\n",
        "What if so much of the data for a record is gone or is somehow incorrect that it really does not add anything to our analysis or worse yet detracts from our analysis? In that case we may want to think about removing that record from our dataset. While this approach should be used as a last resort, it is very easy to implement. For demonstration purposes, we will show how to select records and remove them from our geodataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpY2FMxfdFaF"
      },
      "outputs": [],
      "source": [
        "#let's drop all records with a nan value\n",
        "cdata=gdf_m.dropna()\n",
        "\n",
        "print('The number of rows in cdata4 =',cdata.shape[0])\n",
        "\n",
        "#now let's drop all records with a value of '0.0' in the red_cat column\n",
        "gdf_m3=cdata[cdata['red_cat']!='0.0'].reset_index(drop=True)\n",
        "\n",
        "print('The number of rows in gdf_m3 =',gdf_m3.shape[0])\n",
        "\n",
        "#why do we need to remove na values?\n",
        "\n",
        "#why is dropping rows used as last resort?\n",
        "\n",
        "#How can you change a know value that is incorrect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZxkzN2ldFaF"
      },
      "source": [
        "### Section 6: Preprocessing\n",
        "#### In this section we will begin looking at ways to prepare our data for modeling. While we have 3 datasets to work with (gdf_m1, gdf_m2, and gdf_m3) that have used different cleaning methods to address missing or incorrect data, for the rest of the notebook we will be working with gdf_m3. However, it is worth comparing results against the other cleaned dataset to determine the impact of cleaning.\n",
        "\n",
        "#### Key learning points:\n",
        "- Standardizing, scaling, normalizing, and encoding data\n",
        "- Ordination\n",
        "- Looking for patterns in the data\n",
        "\n",
        "#### Coding steps:\n",
        "1. Standardizing data\n",
        "2. Ordination\n",
        "3. Looking for patterns in the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1: Standardizing, scaling, normalizing, and encoding data\n",
        "Data can have a wide range of values and meaning. To prepare data for modeling, we often need to standardize the data so that no one feature or column is more important than another.Depending on the type of data (categorical or continuous), we have multiple options when it comes to standardization, scaling, normalization, and encoding [SciKit Learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#). For example,  one can normalize continuous data to have zero mean and unit variance or they can simply scale values to range between 0 and 1. Likewise, many modeling techniques require categorical variables to be encoded in a special manner (e.g., One Hot Encoder). In this step we will demonstrate how to scale our Landsat values between 0 and 1 and use the One Hot Encoder method to encode our red_cat variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import modules\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler \n",
        "\n",
        "#extract continuous variables Landsat 1-7\n",
        "X=gdf_m3[gdf_m3.columns[:7]]\n",
        "\n",
        "#create MinMaxScaler object\n",
        "mms=MinMaxScaler()\n",
        "\n",
        "#fit the data\n",
        "mms.fit(X)\n",
        "\n",
        "#Transform the data\n",
        "sX=mms.transform(X)\n",
        "\n",
        "#Convert the transformed data to a dataframe\n",
        "sdf1=pd.DataFrame(sX,columns='sc_'+mms.get_feature_names_out())\n",
        "\n",
        "#extract the categorical variables red_cat\n",
        "X2=gdf_m3[['red_cat']]\n",
        "\n",
        "#create OneHotEncoder\n",
        "ohe=OneHotEncoder()\n",
        "\n",
        "#fit the data\n",
        "ohe.fit(X2)\n",
        "\n",
        "#Transform the data\n",
        "oheX=ohe.transform(X2).toarray()\n",
        "\n",
        "#Convert the transformed data into a dataframe\n",
        "sdf2=pd.DataFrame(oheX,columns=ohe.get_feature_names_out())\n",
        "\n",
        "#Combine the dataframes and look at the data\n",
        "gdf_m3=pd.concat([gdf_m3,sdf1,sdf2],axis=1)\n",
        "\n",
        "#Look at the dataframe\n",
        "gdf_m3\n",
        "\n",
        "# Task 1: use the Standard Scaler to standardize Landsat Values\n",
        "# Task 2: use Ordinal Encoder on forest_density\n",
        "# Task 3: please describe at least one additional technique to standardize your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Ordination\n",
        "Ordination attempts to project and graphically summarize complex relationships. It can be used to reduce the dimensionality of data and or project the data to highlight various aspects of the data. Using our scaled data we will use a ordination technique called principal component analysis [PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca) to make orthogonal predictors and evaluate how much information is in each components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import sklearn PCA decomposition module\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#create a PCA object\n",
        "pca=PCA()\n",
        "\n",
        "#fit the scaled Landsat data (sdf1)\n",
        "pca.fit(sdf1)\n",
        "\n",
        "#Look at the loading values components\n",
        "display(pd.DataFrame(pca.components_,columns=pca.get_feature_names_out()))\n",
        "\n",
        "#Look at the % variance explained by each component\n",
        "display(pd.DataFrame(pca.explained_variance_ratio_,index=pca.get_feature_names_out(),columns=['e_var']).plot(kind='bar',use_index=True))\n",
        "print('% variance explained\\n',pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create a dataframe of transformed values\n",
        "pca_df=pd.DataFrame(pca.transform(sdf1),columns=pca.get_feature_names_out())\n",
        "\n",
        "#Add those values back to our gdf_m3 dataframe\n",
        "gdf_m3=pd.concat([gdf_m3,pca_df],axis=1)\n",
        "\n",
        "#Look at our dataframe\n",
        "gdf_m3\n",
        "\n",
        "#What pca columns should we keep?\n",
        "#Should we standardize our Landsat values or scale them?\n",
        "#What do the loadings mean?\n",
        "#what is another ordination technique we could use?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 3: Looking for patterns in the data\n",
        "Often patterns are easier to see in data that has been transformed. Let's build two graphs to look at patterns; 1) scatter plot matrix of transformed component values and 2) a scatter plot of forest density classes by pca0 and pca1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scatter plot matrix of continuous values\n",
        "pd.plotting.scatter_matrix(gdf_m3[list(gdf_m3.columns[-7:])+['tcc_1']],figsize=(15,15))\n",
        "\n",
        "#Is there trending in the predictor variables?\n",
        "#Was there trending in the predictor variables? If so which variables?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scatter plot of pca0 by pca1 with forest density as a color\n",
        "\n",
        "col={'low':'blue','medium':'green','high':'red'}\n",
        "\n",
        "print('PCA 0 by PCA 1 with forest density classes colored blue(low), green(medium), red(high)')\n",
        "gdf_m3.plot(kind='scatter',x='pca0',y='pca1',color=gdf_m3['forest_density'].replace(col),figsize=(15,15))\n",
        "\n",
        "# Are low, medium, and high densities grouped?\n",
        "# Why did we just plot PCA0 by PCA1?\n",
        "# What proportion of the information does PCA0 and PCA1 account for in the data?\n",
        "# What other techniques could we use to transform our data? \n",
        "# Which variables should we use in our model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKVSiigPdFaF"
      },
      "source": [
        "### Section 7: Building a predictive model\n",
        "#### In this section we will explore how to build a model that can be used later to predict new estimates. While there are many different modeling techniques that can be used to group data or draw a relationship between response and predictor variables, we can generally split these techniques into three groups: 1) clustering, 2) regression, and 3) classification. \n",
        "\n",
        "- Clustering is used to group observations with similar values and can include both continuous and categorical variables.\n",
        "- Regression is used to estimate a continuous response value. Predictor variables can be either continuous or categorical.\n",
        "- Classification is used to estimate a categorical response value. Predictors can be either continuous or categorical.\n",
        "\n",
        "To demonstrate each instance, we will use the following techniques to group like observations, estimate tcc, and classify tree density:\n",
        "- K-means for clustering\n",
        "- Random Forest for regression \n",
        "- Multinomial logistic regression for a classification\n",
        "\n",
        "While there are many different modeling techniques one can use, it is very important to understand the underlying assumptions of a given model and when one modeling technique would be preferred to another technique. For a full description of each modeling technique please refer to the scientific literature. \n",
        "\n",
        "#### Key learning points:\n",
        "- Training, testing, and validation\n",
        "- Model inference\n",
        "- Model estimates and error\n",
        "\n",
        "#### Coding steps\n",
        "1. Training, testing, and validation\n",
        "2. Model inference\n",
        "3. Model estimates and error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1: Training, testing, and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Model inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 3: Model estimates and error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBQ3ZycSdFaF"
      },
      "source": [
        "### Section 8: Estimating % Tree Canopy Cover\n",
        "#### In this section we will explore how to use a model to create new estimates and surfaces\n",
        "\n",
        "#### Key learning points:\n",
        "- Estimates may not reflect reality\n",
        "- The power of more estimates\n",
        "- Aggregating values\n",
        "- Estimates of error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "One import issue that is often overlooked when modeling is determining when a model is interpolating vs extrapolating. So what does this mean? First, let's define these two cases. Interpolating means that predictions are made for new observations in which the predictor values fall within the range of values used to train the model. While, extrapolation occurs when the predictor values used to estimate the new observation fall outside of the range used to calibrate the model. In all cases interpolated estimates are preferred to extrapolated estimate. So what does this mean with regards to the number of variables used to train a model and the models ability to be generalized to the population? Generally, this means that as you increase the number of variables used in a model, you also need to increase the number of observation used to train the model to insure capturing the range of all potential predictor variable values(domain). This situation is often referred to as the curse of dimensionality. Can you think of other issues associated with increasing the number of predictor variables used in a model? \n",
        "\n",
        "When designing a sample or training set, special effort should be made to insure your sample is well spread and balanced to the naturally occurring distribution of the population. Additionally, once a model has been built it is always a good idea to define the range in which estimates are being interpolated vs extrapolating. To do this let's assume we will be using all 7 bands from our Landsat Image as our predictor variables. Using our sample of 1983 observations from dataframe (gdf_m3) we can select build a simple model that queries all cells within our Landsat Image that fall within the range of sampled values using the values returned from describing our data as follows.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 9: Making a dashboard\n",
        "#### In this section we will explore how to convert your results into a functioning dashboard\n",
        "\n",
        "#### Key learning points:\n",
        "- Displaying results\n",
        "- Guiding the user\n",
        "- Hosting"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
